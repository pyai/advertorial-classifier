{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/jupyter/gitlab/advertorial-classifier/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from advertorial import dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "#import wandb\n",
    "import numpy as np\n",
    "#import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# advertorial_dataset = dataset.train_valid_test_from_file(csv_file_path= './data/milelens_advertorial_dataset_formatted.csv')\n",
    "# train, validation, test = advertorial_dataset['train'], advertorial_dataset['validation'], advertorial_dataset['test'] \n",
    "# id2label = {0: \"no\", 1: \"yes\"}\n",
    "# label2id = {\"no\": 0, \"yes\": 1}\n",
    "\n",
    "# pretrain_model =\"hfl/chinese-bert-wwm-ext\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(pretrain_model)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     pretrain_model, num_labels=2, id2label=id2label, label2id=label2id)\n",
    "\n",
    "\n",
    "advertorial_dataset = dataset.train_valid_test_from_file(csv_file_path= './data/unseen_2023-05-30.csv', train_ratio=1)\n",
    "train = advertorial_dataset['train']\n",
    "id2label = {0: \"no\", 1: \"yes\"}\n",
    "label2id = {\"no\": 0, \"yes\": 1}\n",
    "\n",
    "pretrain_model =\"hfl/chinese-bert-wwm-ext\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain_model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrain_model, num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from advertorial.inference import AdvertorialModel\n",
    "adv = AdvertorialModel(model_path='./prebuilt_model/230527_chinese_bert_wwm_ext', use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def perf_report(model, dataset, name='train'):\n",
    "    from tqdm import tqdm\n",
    "    N = len(dataset)\n",
    "    step = 20\n",
    "    ones = 0\n",
    "    zeros = 0\n",
    "    hits = 0\n",
    "    miss = 0\n",
    "    predictions = []\n",
    "    for s in tqdm(range(0, N, step)):\n",
    "        s, e = s, s+step\n",
    "        prediction, probs = model(dataset[s:e]['text'])\n",
    "        \n",
    "        hits += np.sum(dataset[s:e]['label'] == prediction)\n",
    "        miss += np.sum(dataset[s:e]['label'] != prediction)\n",
    "        zeros += np.sum(dataset[s:e]['label'] == np.array(0))\n",
    "        ones += np.sum(dataset[s:e]['label'] == np.array(1))\n",
    "        predictions.append(prediction) \n",
    "\n",
    "    accuracy = hits/N\n",
    "    print(f'accuracy:{accuracy:.2f}, positive samples:{ones}, negative samples:{zeros}')  \n",
    "    performance_df = pd.DataFrame({'dataset':[name], \n",
    "                                   'records':[N], \n",
    "                                   'positive samples':[ones], \n",
    "                                   'negative samples':[zeros], \n",
    "                                   'hit':[hits],\n",
    "                                   'miss':[miss],\n",
    "                                   'accuracy':[accuracy], 'miss rate':[1-accuracy]})\n",
    "\n",
    "    predictions = np.concatenate(predictions)\n",
    "    error_ids = predictions != dataset['label']\n",
    "    error_df = pd.DataFrame({'text':np.array(dataset['text'])[error_ids], 'label':np.array(dataset['label'])[error_ids], 'prediction':predictions[error_ids]})\n",
    "    return error_df, performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [01:21<00:00,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.80, positive samples:1756, negative samples:3013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_error, train_perf = perf_report(adv, train, 'train')\n",
    "#validation_error, validation_perf = perf_report(adv, validation, 'validation')\n",
    "#test_error, test_perf = perf_report(adv, test, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>records</th>\n",
       "      <th>positive samples</th>\n",
       "      <th>negative samples</th>\n",
       "      <th>hit</th>\n",
       "      <th>miss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>miss rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>4769</td>\n",
       "      <td>1756</td>\n",
       "      <td>3013</td>\n",
       "      <td>3828</td>\n",
       "      <td>941</td>\n",
       "      <td>0.802684</td>\n",
       "      <td>0.197316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  records  positive samples  negative samples   hit  miss  accuracy   \n",
       "0   train     4769              1756              3013  3828   941  0.802684  \\\n",
       "\n",
       "   miss rate  \n",
       "0   0.197316  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.concat([train_perf, validation_perf, test_perf]).reset_index(drop=True)#.to_csv('performance.csv', index=False)\n",
    "pd.concat([train_perf]).reset_index(drop=True)#.to_csv('performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advertorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
