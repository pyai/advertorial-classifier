{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ca1bd9",
   "metadata": {},
   "source": [
    "# New flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23d0e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire\n",
    "import os\n",
    "os.chdir('/home/jupyter/gitlab/advertorial-classifier')\n",
    "import sys\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "#os.chdir('../../advertorial-classifier/')\n",
    "#import sys\n",
    "#sys.path.insert(0, )\n",
    "\n",
    "# %%\n",
    "from advertorial import dataset\n",
    "from advertorial import utils\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import wandb\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "epochs=10\n",
    "envfile:str='.env'\n",
    "use_wandb:bool=True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69e715b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env var: MODEL_PROJECT=advertorial-post-classifier\n",
      "env var: GCP_PROJECT=milelens-dev\n",
      "env var: GCP_REGION=asia-east1\n",
      "env var: GCP_BQ_DATASET=ML\n",
      "env var: GCP_BQ_TRAIN_TABLE=advertorial_post_classifier_train\n",
      "env var: GCP_BQ_TEST_TABLE=advertorial_post_classifier_test\n",
      "env var: GCP_BQ_META_TABLE=advertorial_post_classifier_meta\n",
      "env var: GCS_BUCKET=milelens_ml\n",
      "env var: GCS_MODEL_URI_BASE=gs://milelens_ml/advertorial_post_classification/\n",
      "env var: GCV_AI_PIPELINE=gs://milelens_ml/advertorial_post_classification/pipeline\n",
      "env var: WANDB_BASE_URL=http://34.134.95.35\n",
      "env var: WANDB_KEY=local-2ac8bc271378c1f3e406c23d96af534182bdee32\n",
      "env var: WANDB_PROJECT=advertorial-post-classifier\n",
      "use_wandb:True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zhmewiel) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-plant-28</strong> at: <a href='http://34.134.95.35/employee-training/advertorial-post-classifier/runs/zhmewiel' target=\"_blank\">http://34.134.95.35/employee-training/advertorial-post-classifier/runs/zhmewiel</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231019_074249-zhmewiel/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Upgrade to the 0.44.1 version of W&B Server to get the latest features. Learn more: <a href='https://wandb.me/server-upgrade' target=\"_blank\">https://wandb.me/server-upgrade</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zhmewiel). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/gitlab/advertorial-classifier/wandb/run-20231019_075721-uxhhnnu9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://34.134.95.35/employee-training/advertorial-post-classifier/runs/uxhhnnu9' target=\"_blank\">true-plant-29</a></strong> to <a href='http://34.134.95.35/employee-training/advertorial-post-classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://34.134.95.35/employee-training/advertorial-post-classifier' target=\"_blank\">http://34.134.95.35/employee-training/advertorial-post-classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://34.134.95.35/employee-training/advertorial-post-classifier/runs/uxhhnnu9' target=\"_blank\">http://34.134.95.35/employee-training/advertorial-post-classifier/runs/uxhhnnu9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env var: MODEL_PROJECT=advertorial-post-classifier\n",
      "env var: GCP_PROJECT=milelens-dev\n",
      "env var: GCP_REGION=asia-east1\n",
      "env var: GCP_BQ_DATASET=ML\n",
      "env var: GCP_BQ_TRAIN_TABLE=advertorial_post_classifier_train\n",
      "env var: GCP_BQ_TEST_TABLE=advertorial_post_classifier_test\n",
      "env var: GCP_BQ_META_TABLE=advertorial_post_classifier_meta\n",
      "env var: GCS_BUCKET=milelens_ml\n",
      "env var: GCS_MODEL_URI_BASE=gs://milelens_ml/advertorial_post_classification/\n",
      "env var: GCV_AI_PIPELINE=gs://milelens_ml/advertorial_post_classification/pipeline\n",
      "env var: WANDB_BASE_URL=http://34.134.95.35\n",
      "env var: WANDB_KEY=local-2ac8bc271378c1f3e406c23d96af534182bdee32\n",
      "env var: WANDB_PROJECT=advertorial-post-classifier\n",
      "SELECT post_text AS text, cate AS label FROM `milelens-dev.ML.advertorial_post_classifier_train`\n",
      "SELECT post_text AS text, cate AS label FROM `milelens-dev.ML.advertorial_post_classifier_test`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 49198/49198 [00:19<00:00, 2532.78 examples/s]\n",
      "Map: 100%|██████████| 12048/12048 [00:04<00:00, 2518.31 examples/s]\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='89' max='30750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   89/30750 01:14 < 7:19:45, 1.16 it/s, Epoch 0.03/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the advertorial classifier model by train/valid set stored in BQ and log the metrics in wandb.\n",
    "To check the environment variables, please check \n",
    "\n",
    "Args:\n",
    "    envfile (str, optional): Environment variables art listed in here. Defaults to '.env'.\n",
    "\n",
    "Returns:\n",
    "    _type_: None\n",
    "\"\"\"\n",
    "# check or set environment variables\n",
    "utils.check_env(envfile)\n",
    "today = utils.set_today()\n",
    "log_dir = utils.get_based_path('log/')\n",
    "prebuilt_dir = utils.get_based_path('prebuilt_model/')\n",
    "\n",
    "print(f'use_wandb:{use_wandb}')\n",
    "wandb.login(key=os.environ['WANDB_KEY'], \n",
    "            host=os.environ['WANDB_BASE_URL'])\n",
    "wandb.init(\n",
    "    mode= \"online\" if use_wandb else \"disabled\",\n",
    "    project=os.environ['WANDB_PROJECT'],\n",
    "    config={'epochs':epochs}\n",
    ")\n",
    "\n",
    "advertorial_dataset = dataset.train_valid_test_from_file()\n",
    "id2label = {0: \"no\", 1: \"yes\"}\n",
    "label2id = {\"no\": 0, \"yes\": 1}\n",
    "\n",
    "pretrain_model =\"hfl/chinese-bert-wwm-ext\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain_model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrain_model, num_labels=2, id2label=id2label, label2id=label2id)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_advertorial = advertorial_dataset.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=log_dir,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_advertorial[\"train\"],\n",
    "    eval_dataset=tokenized_advertorial[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c4748",
   "metadata": {},
   "source": [
    "# Old flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f7388-9cab-4339-b87c-8dc1bd766ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/jupyter/gitlab/advertorial-classifier/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8d36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from advertorial import dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "#import wandb\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datetime import date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e25a8-1c2f-4e56-b942-4a56fbfef6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# advertorial_dataset = dataset.train_valid_test_from_file(csv_file_path= './data/milelens_advertorial_dataset_formatted.csv')\n",
    "# train, validation, test = advertorial_dataset['train'], advertorial_dataset['validation'], advertorial_dataset['test'] \n",
    "# id2label = {0: \"no\", 1: \"yes\"}\n",
    "# label2id = {\"no\": 0, \"yes\": 1}\n",
    "\n",
    "# pretrain_model =\"hfl/chinese-bert-wwm-ext\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(pretrain_model)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     pretrain_model, num_labels=2, id2label=id2label, label2id=label2id)\n",
    "\n",
    "\n",
    "train_ratio, validation_ratio, test_ratio = 0.8, 0.2, 0\n",
    "advertorial_dataset = dataset.train_valid_test_from_file(csv_file_path= './data/milelens_advertorial_dataset_formatted_23634.csv', train_ratio=train_ratio, validation_ratio=validation_ratio, test_ratio=test_ratio)\n",
    "today = date.today()\n",
    "\n",
    "train = advertorial_dataset['train']\n",
    "train.to_csv(f'./data/train_set_{today}.csv')\n",
    "\n",
    "if validation_ratio:\n",
    "    valid = advertorial_dataset['valid']\n",
    "    valid.to_csv(f'./data/valid_set_{today}.csv')\n",
    "\n",
    "if test_ratio:\n",
    "    test = advertorial_dataset['valid']\n",
    "    test.to_csv(f'./data/test_set_{today}.csv')\n",
    "\n",
    "\n",
    "\n",
    "id2label = {0: \"no\", 1: \"yes\"}\n",
    "label2id = {\"no\": 0, \"yes\": 1}\n",
    "\n",
    "pretrain_model =\"hfl/chinese-bert-wwm-ext\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain_model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrain_model, num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_advertorial = advertorial_dataset.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9458ab-1365-426b-a540-f8cb3860fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    #logging_steps=10000,\n",
    "    #save_steps=10000,\n",
    "    output_dir=\"prebuilt_model/log\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    #evaluation_strategy=\"steps\"\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    #fp16=True,\n",
    "    #load_best_model_at_end=True,\n",
    "    #push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_advertorial[\"train\"],\n",
    "    eval_dataset=tokenized_advertorial[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_advertorial[\"train\"],\n",
    "#     eval_dataset=tokenized_advertorial[\"train\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
